# 情報理論 チートシート

---

## 1. 情報量と情報源

### 重要公式

* **自己情報量 (Self-Information)**: 事象 $s_k$ が生起した時に得られる情報量。
    $I(s_k) = -\log_2 P(s_k)$ \[bit]

* **エントロピー (Entropy)**: 情報源から得られる平均情報量。
    $H(S) = -\sum_{k=1}^{K} P(s_k) \log_2 P(s_k)$ \[bit/symbol]
  * **性質**: $0 \le H(S) \le \log_2 K$。$P(s_k)$ が全て等しいとき（等確率のとき）に最大値 $\log_2 K$ をとる。

* **結合エントロピー (Joint Entropy)**: $H(X, Y)$
    $H(X, Y) = -\sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i, y_j) \log_2 P(x_i, y_j)$

* **条件付きエントロピー (Conditional Entropy)**: $H(Y|X)$
    $H(Y|X) = -\sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i, y_j) \log_2 P(y_j|x_i)$

### 基本用語

* **情報量**: ある事象が起こる確率が低いほど、その事象が起こったときに得られる情報量は大きい。
* **記憶のない情報源**: 各記号の生起が、それ以前に生起した記号とは独立である情報源。

---

## 2. 情報源符号化

情報源から出力される記号を効率的に表現するための符号化。目標は平均符号長 $L$ をエントロピー $H(S)$ に近づけること。

### 重要公式

* **平均符号長 (Average Codeword Length)**:
    $L = \sum_{k=1}^{K} P(s_k) l_k$
  * $l_k$ は記号 $s_k$ の符号語長。

* **クラフトの不等式 (Kraft's Inequality)**: 瞬時復号可能な符号が存在するための必要十分条件。
    $\sum_{k=1}^{K} D^{-l_k} \le 1$
  * $D$ は符号アルファベットのサイズ（2元符号なら $D=2$）。

* **情報源符号化定理**:
    $H(S) \le L < H(S) + 1$
  * 平均符号長 $L$ はエントロピー $H(S)$ より小さくはできない。

* **符号化効率 ($\eta$)**:
    $\eta = \frac{H(S)}{L}$

### 主要アルゴリズム

* **ハフマン符号化 (Huffman Coding)**: 最適なプレフィックス符号を構成するアルゴリズム。
    1. 全ての情報源記号を、生起確率の降順に並べる。
    2. 最も確率の低い2つの記号をまとめ、その合計確率を持つ新しい節点（ノード）を作る。
    3. 新しい節点を再び確率のリストに加え、ソートする。
    4. リストに1つの節点しか残らなくなるまで、ステップ2と3を繰り返す。
    5. 完成した木（ハフマン木）の枝に 0 と 1 を割り当て、根から各記号の葉までのパスをたどることで符号語が完成する。

### 基本用語

* **一意復号可能**: 任意の符号語の系列が、一意に元の記号系列に復号できること。
* **瞬時復号可能 (プレフィックス符号)**: どの符号語も、他の符号語の接頭部（プレフィックス）になっていない符号。符号系列の終点を待たずに先頭から一意に復号できる。

---

## 3. 通信路と相互情報量

### 重要公式

* **相互情報量 (Mutual Information)**: 受信記号 $Y$ を得たことで得られる、送信記号 $X$ に関する情報量。
  * $I(X;Y) = H(X) - H(X|Y)$
  * $I(X;Y) = H(Y) - H(Y|X)$
  * $I(X;Y) = H(X) + H(Y) - H(X,Y)$

* **通信路容量 (Channel Capacity)**: 通信路が誤りなしに伝送できる情報量の最大値。入力確率分布 $P(X)$ を最適化することで求める。
    $C = \max_{P(X)} I(X;Y)$

* **各種通信路の容量**
  * **2元対称通信路 (BSC)**: 誤り率 $p$ の通信路。
      $C = 1 - H(p) = 1 - (-p \log_2 p - (1-p) \log_2 (1-p))$
  * **2元消失通信路 (BEC)**: 消失確率 $\epsilon$ の通信路。
      $C = 1 - \epsilon$
  * **Z通信路**: 0は誤らないが、1が0に誤る確率 $p$ がある通信路。
        $C = \log_2(1 + (1-p)^{1-p} p^p)$

### 基本用語

* **通信路 (Channel)**: 情報を送信側から受信側へ伝達する媒体。
* **通信路行列**: 入力と出力の関係を条件付き確率 $P(y_j|x_i)$ で表した行列。
* **通信路容量 (Channel Capacity)**: その通信路で信頼性を保ちながら情報を伝送できる速度の上限。

---

## 4. 通信路符号化

通信路で発生する誤りを検出・訂正するための符号化。

### 重要公式

* **ハミング距離 (Hamming Distance)**: 2つの同じ長さの符号語間で、対応する位置の記号が異なる箇所の数。$d(u,v)$ で表す。
* **最小ハミング距離 ($d_{min}$)**: 符号に含まれる全ての異なる符号語対のハミング距離の最小値。
* **誤り検出・訂正能力**
  * **$e$ ビットの誤り検出**: $d_{min} \ge e + 1$
  * **$t$ ビットの誤り訂正**: $d_{min} \ge 2t + 1$

* **線形符号 (Linear Codes)**: $(n, k)$ 符号。$k$ ビットの情報に $n-k$ ビットの冗長ビットを加えて $n$ ビットの符号語を生成する。
  * **生成行列 (Generator Matrix, $G$)**: $k \times n$ 行列。情報ベクトル $u$ から符号語 $c$ を生成する。 $c = uG$
  * **パリティ検査行列 (Parity Check Matrix, $H$)**: $(n-k) \times n$ 行列。$GH^T = 0$ を満たす。
  * **シンドローム (Syndrome, $S$)**: 受信語 $r$ から計算されるベクトル。 $S = rH^T$

### 主要アルゴリズム

* **シンドローム復号 (Syndrome Decoding)**
    1. 受信語 $r$ を受け取る。
    2. シンドローム $S = rH^T$ を計算する。
    3. $S=0$ ならば、誤りなしと判断し $r$ をそのまま出力する。
    4. $S \ne 0$ ならば、シンドローム $S$ に対応する誤りパターン $e$ を見つける。
    5. 受信語から誤りパターンを引く（2元符号ではXOR）ことで、元の符号語を推定する。 $\hat{c} = r + e$

### 基本用語

* **符号語 (Codeword)**: 情報ビットに冗長ビットを付加して作られた伝送単位。
* **ハミング重み**: 符号語に含まれる "1" の数。
* **シンドローム**: 誤りのパターンを特定するための手がかりとなるベクトル。誤りがなければゼロベクトルになる。
